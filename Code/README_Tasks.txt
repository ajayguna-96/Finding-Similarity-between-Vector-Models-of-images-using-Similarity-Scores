TASK SOLUTION DESCRIPTION

The schema for the first 3 tasks (content of corresponding CSV Files) are stored as below:
				[Annotations UserID1 UserID2 UserID3]
				["wicked"      Value   Value   Value]
            			[......  	....    ....    ....]
				
	At first, all the tasks were performed using PostgreSQL DB. The computation time of the task perfomed using PostgreSQL was found to be significantly longer than that performed using CSV files. Hence, we stuck to using CSV files to store and retrieve data quickly. A NoSQL Document based DB like MongoDB hasn't been tested and has been left to the readers to download the codebase and try to implement the task using MongoDB. Do compare the results and get back to me on how that goes. :)

Finding User Similarity based on Textual Descriptors (Task 1):
	The task requires us to extract 'k' similar users for a given user id based on the tf, df or tf-idf values as specified in the sample inputs file. The data is extracted from the “devset_textTermsPerUser.txt” file and written in the required schema format specified above i.e. The column header as the user id's of the various users and the row headers as the annotations.

	The input is first read row by row such that you have the transpose of the above format and then made into the required format (Check the format of the above mentioned text file in the dataset for more clarity). A dictionary is used for faster mapping of user annotations (See code). The TF, DF and TF-IDF values are stored as separate CSV files to access when required (Check Code/CSV/Task_1 after running "CSV_Writer_DB_User.py"). To find the similarity, we load the required CSV file into a pandas data frame and find the cosine similarity between the columns (User ID’s) and take the highest similarity score. The most contributing annotations are then found by finding the Manhattan distance (value_1 - value_2) between the features (TF, DF or TF-IDF) for each annotation of a column and finding the lowest 3 differences (highest similarty) found. See outputs for more information.

Finding Image Similarity based on Textual Descriptors (Task 2):
	The task requires us to extract 'k' similar images for a given image id based on the tf, df or tf-idf values as specified in the sample inputs file. The data is extracted from the “devset_textTermsPerImages.txt” file and written in the required format specified above i.e. The column header as the image id's of the various images and the row headers as the annotations.

	The approach is exactly the same as done for the users. The input is first read row by row such that you have the transpose of the above format and then made into the required format (Check the format of the above mentioned text file in the dataset for more clarity). A dictionary is used for faster mapping of image annotations (See code). The TF, DF and TF-IDF values are stored as separate CSV files to access when required (Check Code/CSV/Task_2 after running "CSV_Writer_DB_Images.py"). To find the similarity, we load the required CSV file into a pandas data frame and find the cosine similarity between the columns (Image ID’s) and take the highest similarity score. The most contributing annotations are then found by finding the Manhattan distance (value_1 - value_2) between the features (TF, DF or TF-IDF) for each annotation of a column and finding the lowest 3 differences (highest similarty) found. See outputs for more information.

Finding Location Similarity based on Textual Descriptors (Task 3):
	The task requires us to extract 'k' similar locations for a given location id based on the tf, df or tf-idf values as specified in the sample inputs file. The data is extracted from the devset_textTermsPerPOI.txt” file and written in the required format specified above i.e. The column header as the location id's of the various locations and the row headers as the annotations.

	To eliminate the ambiguity of length of the location name at the start of each row, we extracted the line till we got a quotes (' " ') [ See dataset for more clarity] and then joined the extracted words with an underscore and wrote another file named “devset_textTermsPerPOIEdited.txt” The input is first read row by row such that you have the transpose of the above format and then made into the required format (Check the format of the above mentioned text file in the dataset for more clarity). A dictionary is used for faster mapping of location annotations (See code). The TF, DF and TF-IDF values are stored as separate CSV files to access when required (Check Code/CSV/Task_3 after running "CSV_Writer_DB_Locations.py"). To find the similarity, we load the required CSV file into a pandas data frame and find the cosine similarity between the columns (Location ID’s) and take the highest similarity score. The most contributing annotations are then found by finding the Manhattan distance (value_1 - value_2) between the features (TF, DF or TF-IDF) for each annotation of a column and finding the lowest 3 differences (highest similarty) found. See outputs for more information.


Finding Location Similarity based on Visual Descriptors and given model (Task 4):
	Task 4 requires the system to find the 'k' most similar locations given a location id and model name(CM, CM3x3, etc). The brute force method to completing this task would be to take an image from the given location ID and to compare its similarity with all other images in another location and finding the most similar image using cosine similarity measure and mapping that image with this image. This is done for all the images of the given location and another location and then repeated again for every other location. The similarity score between locations is found by adding the maximum similarity score found for all the images between the given location and every other location. The similarity sum is then sorted in descending order to get the top 'k' similar locations. We perform clustering on images in a location because the original brute force method took a significant amount of time for 300 images per location. A K-Means Clustering is done with a cluster size of 10 with 10 image samples taken from each cluster and written into the "CSV/img_edit" folder (Check Tasks/K_Means_Clustering.py). The optimal cluster size was found using elbow method. These images are used for Task_4 which gives a faster and more efficient result with very little change in the similarity scores. Another optimization that was performed was that, everytime an image from the given location finds a most similar image in the other location that is being compared, the image from the second location is removed from the list of images for future comparisons. This still keeps the complexity as n^2 but significantly reduces number of comparisons for large datasets and at the same time prevents redundancies. The most contributing images between two locations are taken into account by taking the maximum cosine similarity score between two images of the corresponding locations.
	P.S.: We also performed an alternative approach which was to choose 100 images randomly and compared only those images to find the similar locations but we found a very deviating result from the brute force one and hence refrained from using it.
	We also thought about using binary search to find the most similar image of one location in another location and that again would involve sorting based on some parameter which would result in an O(nlogn) time complexity. But due to time constraints we weren't able to develop on this idea to find a parameter on which the similarity can be judged and sorted upon so that everytime we move in a direction we could be sure that we would converge on the most similar image at the end of the process. We leave it to the readers to explore and devise a solution. ;)

Finding Location Similarity based on Visual Descriptors and all models (Task 5):
	Task 5 requires the system to find the 'k' most similar locations given only the location id. This would be a continuation of Task 4 described above. We would get the most similar locations using a particular model using the exact same code of Task 4. What is different about Task 5 is that we would have to repeat Task 4 for all the models given and get the output for each model. We then compare the outputs of each model and count the common locations given by each model (Each model gives 'k' most similar locations). The task performs K-Means Clustering with a cluster size of 7 with 10 image samples taken from each cluster and written into the "CSV/img_edit_task5" folder (Check Tasks/K_Means_Clustering_Task_5.py). The cluster size is reduced for this task to reduce the computation time. These images are used for Task_5. The individual contribution of each model and the corresponding rank for that location given by the model is also taken into account. Rank given by a model for a location is given weightage when calculating the counts. The top rank gets ‘k’ summed to it, second gets ‘k-1’ summed and so on to get the total count score between the models.
